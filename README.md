# LLM-Dashed-Black-Line-Analysis
Manual data extraction from graphs is a protracted process, and whether Multimodal large language models(MLLMs) can provide enough accuracy and efficiency to replace human labor remains undetermined. With the introduction of vision language fusions, MLLMs such as GPT 5.2, Gemini(1.5), Claude(3), and Deepseek are introduced in the process of extracting and interpreting graphs from images. By turning encoding images into visual tokens, recognizing patches that ensembles lines, labels, numbers, and axis, large language models, these MLLMs are able to extract points on a curve, summarize and provide qualitative data to perform data reconstruction. In this paper, we will evaluate the accuracy of the MLLMs mentioned above by comparing their output with manually extracted dataset from WebPlotDigitizer. Visual comparison and Mean Squared Error will be used to both qualitatively and quantitatively display the accuracy of each MLLM. The result will serve as a measure of the reliability of using MLLMs to replace manual data extraction and analysis. 
